{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from skimage import io\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "from dataset_preparing import get_coords_from_densifing_points, generate_heatmap\n",
    "# dense에 대한 간격이나 표준편차를 하이퍼 파라미터로 조정가능한 코드. 히트맵 생성 때문에 속도는 좀 걸릴 수 있음\n",
    "# Inria 데이터 크기 조정하여 coco 포맷으로 맞춰준 데이터 처리가능\n",
    "\n",
    "def min_max_normalize(image, percentile, nodata=-1.):\n",
    "    image = image.astype('float32')\n",
    "    mask = np.mean(image, axis=2) != nodata * image.shape[2]\n",
    "\n",
    "    percent_min = np.percentile(image, percentile, axis=(0, 1))\n",
    "    percent_max = np.percentile(image, 100-percentile, axis=(0, 1))\n",
    "\n",
    "    if image.shape[1] * image.shape[0] - np.sum(mask) > 0:\n",
    "        mdata = np.ma.masked_equal(image, nodata, copy=False)\n",
    "        mdata = np.ma.filled(mdata, np.nan)\n",
    "        percent_min = np.nanpercentile(mdata, percentile, axis=(0, 1))\n",
    "\n",
    "    norm = (image-percent_min) / (percent_max - percent_min)\n",
    "    norm[norm < 0] = 0\n",
    "    norm[norm > 1] = 1\n",
    "    norm = (norm * 255).astype('uint8') * mask[:, :, np.newaxis]\n",
    "\n",
    "    return norm\n",
    "\n",
    "\n",
    "def image_graph_collate_road_network_coco(batch):\n",
    "    images = torch.stack([item['image'] for item in batch], 0).contiguous()\n",
    "    heatmap = torch.stack([item['heatmap'] for item in batch], 0).contiguous()\n",
    "    points = [item['nodes'] for item in batch]\n",
    "    edges = [item['edges'] for item in batch]\n",
    "\n",
    "    return [images, heatmap, points, edges]\n",
    "\n",
    "\n",
    "def create_polygon(segmentation):\n",
    "    # COCO segmentation format is [x1, y1, x2, y2, ..., xn, yn]\n",
    "    # We need to reshape it to [(x1, y1), (x2, y2), ..., (xn, yn)]\n",
    "    points = list(zip(segmentation[::2], segmentation[1::2]))\n",
    "    return Polygon(points)\n",
    "\n",
    "\n",
    "def gdf_to_nodes_and_edges(gdf):\n",
    "    nodes = []\n",
    "    for _, row in gdf.iterrows():\n",
    "        polygon = row['geometry']\n",
    "        if polygon.geom_type == 'Polygon':\n",
    "            for x, y in polygon.exterior.coords:\n",
    "                nodes.append((x, y))\n",
    "        elif polygon.geom_type == 'MultiPolygon':\n",
    "            for part in polygon:\n",
    "                for x, y in part.exterior.coords:\n",
    "                    nodes.append((x, y))\n",
    "        else:\n",
    "            raise AttributeError\n",
    "\n",
    "    # Remove duplicates if necessary\n",
    "    nodes = list(set(nodes))\n",
    "\n",
    "    # Create a DataFrame for nodes with unique indices\n",
    "    node_df = pd.DataFrame(nodes, columns=['x', 'y'])\n",
    "    node_df['node_id'] = range(len(node_df))\n",
    "\n",
    "    edges = []\n",
    "    for _, row in gdf.iterrows():\n",
    "        polygon = row['geometry']\n",
    "        if polygon.geom_type == 'Polygon':\n",
    "            coords = polygon.exterior.coords[:-1]  # Exclude closing vertex\n",
    "            edge = [(node_df[(node_df['x'] == x) & (node_df['y'] == y)].index[0], \n",
    "                    node_df[(node_df['x'] == coords[(i+1)%len(coords)][0]) & (node_df['y'] == coords[(i+1)%len(coords)][1])].index[0]) \n",
    "                    for i, (x, y) in enumerate(coords)]\n",
    "            edges.extend(edge)\n",
    "        elif polygon.geom_type == 'MultiPolygon':\n",
    "            for part in polygon:\n",
    "                coords = part.exterior.coords[:-1]\n",
    "                edge = [(node_df[(node_df['x'] == x) & (node_df['y'] == y)].index[0], \n",
    "                        node_df[(node_df['x'] == coords[(i+1)%len(coords)][0]) & (node_df['y'] == coords[(i+1)%len(coords)][1])].index[0]) \n",
    "                        for i, (x, y) in enumerate(coords)]\n",
    "                edges.extend(edge)\n",
    "\n",
    "    return node_df[['y', 'x']].values, edges\n",
    "\n",
    "\n",
    "class CrowdAI(Dataset):\n",
    "    \"\"\"A dataset class for handling and processing data from the CrowdAI dataset.\n",
    "\n",
    "    Attributes:\n",
    "        IMAGES_DIRECTORY (str): Directory containing the images.\n",
    "        ANNOTATIONS_PATH (str): File path for the annotations.\n",
    "        coco (COCO): COCO object to handle COCO annotations.\n",
    "        max_points (int): Maximum number of points to consider (default 256).\n",
    "        gap_distance (float): Distance between interpolated points.\n",
    "        sigma (float): Standard deviation for Gaussian kernel used in heatmap generation.\n",
    "\n",
    "    Args:\n",
    "        images_directory (str): Directory where the dataset images are stored.\n",
    "        annotations_path (str): File path for the COCO format annotations.\n",
    "        gap_distance (int, optional): Gap distance for densifying points. Defaults to 20.\n",
    "        sigma (float, optional): Sigma value for Gaussian blur in heatmap. Defaults to 1.5.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 images_directory, \n",
    "                 annotations_path,\n",
    "                 gap_datance=20,\n",
    "                 sigma=1.5):\n",
    "\n",
    "        self.IMAGES_DIRECTORY = images_directory\n",
    "        self.ANNOTATIONS_PATH = annotations_path\n",
    "        self.coco = COCO(self.ANNOTATIONS_PATH)\n",
    "        self.image_ids = self.coco.getImgIds(catIds=self.coco.getCatIds())\n",
    "\n",
    "        self.len = len(self.image_ids)\n",
    "\n",
    "        self.max_points = 256 # TODO: It should be restricted the number when gt points over the max points limit\n",
    "        self.gap_distance = gap_datance\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def prepare_annotations(self, img):\n",
    "        \"\"\"Prepares annotations for an image.\n",
    "        Args:\n",
    "            img (dict): A dictionary containing image metadata.\n",
    "\n",
    "        Returns:\n",
    "            GeoDataFrame: A GeoDataFrame containing the geometrical data of annotations.\n",
    "        \"\"\"\n",
    "        annotation_ids = self.coco.getAnnIds(imgIds=img['id'])\n",
    "        annotations = self.coco.loadAnns(annotation_ids)\n",
    "        random.shuffle(annotations)\n",
    "\n",
    "        data = []\n",
    "        for ann in annotations:\n",
    "            polygon = create_polygon(ann['segmentation'][0])\n",
    "            data.append({'id': ann['id'], 'geometry': polygon})\n",
    "        gdf = gpd.GeoDataFrame(data, geometry='geometry')\n",
    "        return gdf\n",
    "\n",
    "    def loadSample(self, idx):\n",
    "        \"\"\"Loads a sample for a given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to load.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the sample data.\n",
    "                'image' (torch.Tensor of shape [3, H, W], torch.float32): \n",
    "                    The image tensor normalized to [0, 1].\n",
    "                'image_idx' (torch.Tensor of shape [1], torch.long): \n",
    "                    The index of the image.\n",
    "                'heatmap' (torch.Tensor of shape [H, W], torch.float32): \n",
    "                    The heatmap tensor for the image normalized to [0, 1]. \n",
    "                'nodes' (torch.Tensor of shape [N, 2], torch.float): \n",
    "                    The nodes tensor representing points in the image.\n",
    "                    nodes are normalized to [0, 1]\n",
    "                'edges' (torch.Tensor of shape [E, 2], torch.long): \n",
    "                    The edges tensor representing connections between nodes.\n",
    "        \"\"\"\n",
    "        idx = self.image_ids[idx]\n",
    "\n",
    "        img = self.coco.loadImgs(idx)[0]\n",
    "        image_path = os.path.join(self.IMAGES_DIRECTORY, img['file_name'])\n",
    "        image = io.imread(image_path)\n",
    "\n",
    "        gdf = self.prepare_annotations(img)\n",
    "        coords, gdf = get_coords_from_densifing_points(gdf, gap_distance=self.gap_distance) # [N, 2]\n",
    "        heatmap = generate_heatmap(coords, image.shape[:2], sigma=self.sigma)\n",
    "\n",
    "        nodes, edges = gdf_to_nodes_and_edges(gdf)\n",
    "        nodes = nodes / image.shape[0]\n",
    "\n",
    "        image_idx = torch.tensor([idx])\n",
    "        image = torch.from_numpy(image)\n",
    "        image = image.permute(2,0,1) / 255.0\n",
    "        heatmap = torch.from_numpy(heatmap) / 255.0\n",
    "        \n",
    "        nodes = torch.tensor(nodes, dtype=torch.float)\n",
    "        edges = torch.tensor(edges, dtype=torch.long)\n",
    "\n",
    "        sample = {\n",
    "            'image': image, \n",
    "            'image_idx': image_idx, \n",
    "            'heatmap': heatmap,\n",
    "            'nodes': nodes,\n",
    "            'edges': edges\n",
    "            }\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.loadSample(idx)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CrowdAI(images_directory='/nas/tsgil/dataset/Inria_building/cocostyle/images',\n",
    "                    annotations_path='/nas/tsgil/dataset/Inria_building/cocostyle/annotation.json')\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=6, collate_fn=image_graph_collate_road_network_coco)\n",
    "\n",
    "print(next(iter(dataloader))[0].shape) # image\n",
    "\n",
    "data = next(iter(dataloader))\n",
    "\n",
    "image = data[0][1].detach().cpu().numpy().transpose(1,2,0)\n",
    "heatmap =  data[1][1].detach().cpu().numpy()\n",
    "nodes = data[2][1].detach().cpu().numpy() * image.shape[0]\n",
    "edges = data[3][1].detach().cpu().numpy()\n",
    "\n",
    "nodes = nodes.astype('int64')\n",
    "\n",
    "# Visualize\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(min_max_normalize(image, 0))\n",
    "plt.scatter(nodes[:,1], nodes[:,0], color='r')\n",
    "\n",
    "for e in edges:\n",
    "    connect = np.stack([nodes[e[0]], nodes[e[1]]], axis=0)\n",
    "    plt.plot(connect[:,1], connect[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 임의의 사각형 N 타겟 노드와 엣지 리턴 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_directed_adjacency_matrix(pairs_list):\n",
    "    \"\"\" 주어진 노드 쌍 리스트를 기반으로 방향성 인접 행렬을 생성합니다. \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # 인접 행렬 크기 계산 (최대 노드 인덱스 + 1)\n",
    "    max_index = max(max(pair) for pair in pairs_list)\n",
    "    adjacency_matrix = np.zeros((max_index + 1, max_index + 1), dtype=int)\n",
    "\n",
    "    # 인접 행렬 채우기\n",
    "    for i, j in pairs_list:\n",
    "        adjacency_matrix[i, j] = 1  # 방향성 그래프: i에서 j로의 방향\n",
    "\n",
    "    return adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "from dataset_preparing import get_coords_from_densifing_points\n",
    "from dataloader_cocostyle import gdf_to_nodes_and_edges\n",
    "\n",
    "# shapes and dtypes of nodes/edges are exactly same as what I defined in InriaDataLoader\n",
    "\n",
    "original_polygon = Polygon([(0, 0), (100, 0), (100, 100), (0, 100)])\n",
    "# original_polygon = Polygon([(10, 10), (90, 10), (90, 90), (10, 90)])\n",
    "gdf = gpd.GeoDataFrame(geometry = [original_polygon])\n",
    "\n",
    "coords, gdf = get_coords_from_densifing_points(gdf, gap_distance=25)\n",
    "nodes, edges = gdf_to_nodes_and_edges(gdf)\n",
    "nodes = nodes / 100\n",
    "\n",
    "nodes = nodes.astype('float32')\n",
    "edges = np.array(edges).astype('int64')\n",
    "\n",
    "\n",
    "# Visualizer\n",
    "plt.scatter(nodes[:,1], nodes[:,0])\n",
    "for e in edges:\n",
    "    plt.plot(nodes[e,1], nodes[e,0])\n",
    "\n",
    "plt.show()\n",
    "generate_directed_adjacency_matrix(edges)\n",
    "print(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.DGS import HungarianMatcher\n",
    "import torch\n",
    "\n",
    "N = len(nodes)\n",
    "K = 50\n",
    "\n",
    "matcher = HungarianMatcher()\n",
    "matcher.eval()\n",
    "output = { # K개의 샘플링 뽑는 걸로 수정하기\n",
    "    \"pred_nodes\": torch.rand(1, K, 2),\n",
    "    } # x_cord, y_cord\n",
    "target = {'nodes':[torch.tensor(nodes)]}\n",
    "out = matcher(output, target)\n",
    "print(out[0][0])\n",
    "print(out[0][1])\n",
    "mapping = {j:i for i,j in zip(out[0][0].tolist(),out[0][1].tolist())}\n",
    "print(mapping)\n",
    "gt = [nodes[node] for node in out[0][1]]\n",
    "sample = output['pred_nodes'][0]\n",
    "print(gt)\n",
    "print(sample)\n",
    "print(nodes[0],nodes[9])\n",
    "print(edges)\n",
    "plt.scatter(sample[:,1], sample[:,0])\n",
    "if K<N:\n",
    "    mapping2 = {edge[0]: edge[1] for edge in edges}\n",
    "    sample_edge = []\n",
    "    for i, j in edges:\n",
    "        if i not in mapping:\n",
    "            while i not in mapping:\n",
    "                i = mapping2[i]\n",
    "        if j not in mapping:\n",
    "            while j not in mapping:\n",
    "                j = mapping2[j]\n",
    "        if mapping[i] != mapping[j]:\n",
    "            sample_edge.append([mapping[i],mapping[j]])\n",
    "else:\n",
    "    sample_edge = [[mapping[i],mapping[j]] for i, j in edges]\n",
    "for e in sample_edge:\n",
    "    plt.plot(sample[e,1], sample[e,0])\n",
    "plt.show()\n",
    "print(generate_directed_adjacency_matrix(sample_edge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from skimage import io\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "from dataset_preparing import get_coords_from_densifing_points, generate_heatmap\n",
    "# dense에 대한 간격이나 표준편차를 하이퍼 파라미터로 조정가능한 코드. 히트맵 생성 때문에 속도는 좀 걸릴 수 있음\n",
    "# Inria 데이터 크기 조정하여 coco 포맷으로 맞춰준 데이터 처리가능\n",
    "\n",
    "def min_max_normalize(image, percentile, nodata=-1.):\n",
    "    image = image.astype('float32')\n",
    "    mask = np.mean(image, axis=2) != nodata * image.shape[2]\n",
    "\n",
    "    percent_min = np.percentile(image, percentile, axis=(0, 1))\n",
    "    percent_max = np.percentile(image, 100-percentile, axis=(0, 1))\n",
    "\n",
    "    if image.shape[1] * image.shape[0] - np.sum(mask) > 0:\n",
    "        mdata = np.ma.masked_equal(image, nodata, copy=False)\n",
    "        mdata = np.ma.filled(mdata, np.nan)\n",
    "        percent_min = np.nanpercentile(mdata, percentile, axis=(0, 1))\n",
    "\n",
    "    norm = (image-percent_min) / (percent_max - percent_min)\n",
    "    norm[norm < 0] = 0\n",
    "    norm[norm > 1] = 1\n",
    "    norm = (norm * 255).astype('uint8') * mask[:, :, np.newaxis]\n",
    "\n",
    "    return norm\n",
    "\n",
    "\n",
    "def image_graph_collate_road_network_coco(batch):\n",
    "    images = torch.stack([item['image'] for item in batch], 0).contiguous()\n",
    "    heatmap = torch.stack([item['heatmap'] for item in batch], 0).contiguous()\n",
    "    points = [item['nodes'] for item in batch]\n",
    "    edges = [item['edges'] for item in batch]\n",
    "\n",
    "    return [images, heatmap, points, edges]\n",
    "\n",
    "\n",
    "def create_polygon(segmentation):\n",
    "    # COCO segmentation format is [x1, y1, x2, y2, ..., xn, yn]\n",
    "    # We need to reshape it to [(x1, y1), (x2, y2), ..., (xn, yn)]\n",
    "    points = list(zip(segmentation[::2], segmentation[1::2]))\n",
    "    return Polygon(points)\n",
    "\n",
    "\n",
    "def gdf_to_nodes_and_edges(gdf):\n",
    "    nodes = []\n",
    "    for _, row in gdf.iterrows():\n",
    "        polygon = row['geometry']\n",
    "        if polygon.geom_type == 'Polygon':\n",
    "            for x, y in polygon.exterior.coords:\n",
    "                nodes.append((x, y))\n",
    "        elif polygon.geom_type == 'MultiPolygon':\n",
    "            for part in polygon:\n",
    "                for x, y in part.exterior.coords:\n",
    "                    nodes.append((x, y))\n",
    "        else:\n",
    "            raise AttributeError\n",
    "\n",
    "    # Remove duplicates if necessary\n",
    "    nodes = list(set(nodes))\n",
    "\n",
    "    # Create a DataFrame for nodes with unique indices\n",
    "    node_df = pd.DataFrame(nodes, columns=['x', 'y'])\n",
    "    node_df['node_id'] = range(len(node_df))\n",
    "\n",
    "    edges = []\n",
    "    for _, row in gdf.iterrows():\n",
    "        polygon = row['geometry']\n",
    "        if polygon.geom_type == 'Polygon':\n",
    "            coords = polygon.exterior.coords[:-1]  # Exclude closing vertex\n",
    "            edge = [(node_df[(node_df['x'] == x) & (node_df['y'] == y)].index[0], \n",
    "                    node_df[(node_df['x'] == coords[(i+1)%len(coords)][0]) & (node_df['y'] == coords[(i+1)%len(coords)][1])].index[0]) \n",
    "                    for i, (x, y) in enumerate(coords)]\n",
    "            edges.extend(edge)\n",
    "        elif polygon.geom_type == 'MultiPolygon':\n",
    "            for part in polygon:\n",
    "                coords = part.exterior.coords[:-1]\n",
    "                edge = [(node_df[(node_df['x'] == x) & (node_df['y'] == y)].index[0], \n",
    "                        node_df[(node_df['x'] == coords[(i+1)%len(coords)][0]) & (node_df['y'] == coords[(i+1)%len(coords)][1])].index[0]) \n",
    "                        for i, (x, y) in enumerate(coords)]\n",
    "                edges.extend(edge)\n",
    "\n",
    "    return node_df[['y', 'x']].values, edges\n",
    "\n",
    "\n",
    "class CrowdAI(Dataset):\n",
    "    \"\"\"A dataset class for handling and processing data from the CrowdAI dataset.\n",
    "\n",
    "    Attributes:\n",
    "        IMAGES_DIRECTORY (str): Directory containing the images.\n",
    "        ANNOTATIONS_PATH (str): File path for the annotations.\n",
    "        coco (COCO): COCO object to handle COCO annotations.\n",
    "        max_points (int): Maximum number of points to consider (default 256).\n",
    "        gap_distance (float): Distance between interpolated points.\n",
    "        sigma (float): Standard deviation for Gaussian kernel used in heatmap generation.\n",
    "\n",
    "    Args:\n",
    "        images_directory (str): Directory where the dataset images are stored.\n",
    "        annotations_path (str): File path for the COCO format annotations.\n",
    "        gap_distance (int, optional): Gap distance for densifying points. Defaults to 20.\n",
    "        sigma (float, optional): Sigma value for Gaussian blur in heatmap. Defaults to 1.5.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 images_directory, \n",
    "                 annotations_path,\n",
    "                 gap_datance=20,\n",
    "                 sigma=1.5):\n",
    "\n",
    "        self.IMAGES_DIRECTORY = images_directory\n",
    "        self.ANNOTATIONS_PATH = annotations_path\n",
    "        self.coco = COCO(self.ANNOTATIONS_PATH)\n",
    "        self.image_ids = self.coco.getImgIds(catIds=self.coco.getCatIds())\n",
    "\n",
    "        self.len = len(self.image_ids)\n",
    "\n",
    "        self.max_points = 256 # TODO: It should be restricted the number when gt points over the max points limit\n",
    "        self.gap_distance = gap_datance\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def prepare_annotations(self, img):\n",
    "        \"\"\"Prepares annotations for an image.\n",
    "        Args:\n",
    "            img (dict): A dictionary containing image metadata.\n",
    "\n",
    "        Returns:\n",
    "            GeoDataFrame: A GeoDataFrame containing the geometrical data of annotations.\n",
    "        \"\"\"\n",
    "        annotation_ids = self.coco.getAnnIds(imgIds=img['id'])\n",
    "        annotations = self.coco.loadAnns(annotation_ids)\n",
    "        random.shuffle(annotations)\n",
    "\n",
    "        data = []\n",
    "        for ann in annotations:\n",
    "            polygon = create_polygon(ann['segmentation'][0])\n",
    "            data.append({'id': ann['id'], 'geometry': polygon})\n",
    "        gdf = gpd.GeoDataFrame(data, geometry='geometry')\n",
    "        return gdf\n",
    "\n",
    "    def loadSample(self, idx):\n",
    "        \"\"\"Loads a sample for a given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to load.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the sample data.\n",
    "                'image' (torch.Tensor of shape [3, H, W], torch.float32): \n",
    "                    The image tensor normalized to [0, 1].\n",
    "                'image_idx' (torch.Tensor of shape [1], torch.long): \n",
    "                    The index of the image.\n",
    "                'heatmap' (torch.Tensor of shape [H, W], torch.float32): \n",
    "                    The heatmap tensor for the image normalized to [0, 1]. \n",
    "                'nodes' (torch.Tensor of shape [N, 2], torch.float): \n",
    "                    The nodes tensor representing points in the image.\n",
    "                    nodes are normalized to [0, 1]\n",
    "                'edges' (torch.Tensor of shape [E, 2], torch.long): \n",
    "                    The edges tensor representing connections between nodes.\n",
    "        \"\"\"\n",
    "        idx = self.image_ids[idx]\n",
    "\n",
    "        img = self.coco.loadImgs(idx)[0]\n",
    "        image_path = os.path.join(self.IMAGES_DIRECTORY, img['file_name'])\n",
    "        image = io.imread(image_path)\n",
    "\n",
    "        gdf = self.prepare_annotations(img)\n",
    "        coords, gdf = get_coords_from_densifing_points(gdf, gap_distance=self.gap_distance) # [N, 2]\n",
    "        heatmap = generate_heatmap(coords, image.shape[:2], sigma=self.sigma)\n",
    "\n",
    "        nodes, edges = gdf_to_nodes_and_edges(gdf)\n",
    "        nodes = nodes / image.shape[0]\n",
    "\n",
    "        image_idx = torch.tensor([idx])\n",
    "        image = torch.from_numpy(image)\n",
    "        image = image.permute(2,0,1) / 255.0\n",
    "        heatmap = torch.from_numpy(heatmap) / 255.0\n",
    "        \n",
    "        nodes = torch.tensor(nodes, dtype=torch.float)\n",
    "        edges = torch.tensor(edges, dtype=torch.long)\n",
    "\n",
    "        sample = {\n",
    "            'image': image, \n",
    "            'image_idx': image_idx, \n",
    "            'heatmap': heatmap,\n",
    "            'nodes': nodes,\n",
    "            'edges': edges\n",
    "            }\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.loadSample(idx)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CrowdAI(images_directory='/nas/tsgil/dataset/Inria_building/cocostyle/images',\n",
    "                    annotations_path='/nas/tsgil/dataset/Inria_building/cocostyle/annotation.json')\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=6, collate_fn=image_graph_collate_road_network_coco)\n",
    "\n",
    "print(next(iter(dataloader))[0].shape) # image\n",
    "\n",
    "data = next(iter(dataloader))\n",
    "\n",
    "image = data[0][1].detach().cpu().numpy().transpose(1,2,0)\n",
    "heatmap =  data[1][1].detach().cpu().numpy()\n",
    "nodes = data[2][1].detach().cpu().numpy() * image.shape[0]\n",
    "edges = data[3][1].detach().cpu().numpy()\n",
    "\n",
    "nodes = nodes.astype('int64')\n",
    "\n",
    "# Visualize\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(min_max_normalize(image, 0))\n",
    "plt.scatter(nodes[:,1], nodes[:,0], color='r')\n",
    "\n",
    "for e in edges:\n",
    "    connect = np.stack([nodes[e[0]], nodes[e[1]]], axis=0)\n",
    "    plt.plot(connect[:,1], connect[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "gdf = gpd.GeoDataFrame({'k':[1, None]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Example model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 20), # 10->20, 파라미터 20*10개\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10) # 20->10, 파라미터 10*20개\n",
    ")\n",
    "\n",
    "# Print the weights of the first linear layer\n",
    "for name, param in model.named_parameters():\n",
    "    if name == '0.weight':  # '0' refers to the first layer\n",
    "        print(param)\n",
    "a = model.state_dict()\n",
    "print(a.keys())\n",
    "a['2.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.TopDiG import build_TopDiG\n",
    "from models.backbone_R2U_Net import R2U_Net, R2U_Net_origin\n",
    "ck_TopDiG = torch.load('/nas/tsgil/relationformer/work_dirs/TopDiG_train/runs/baseline_TopDiG_train_epoch8_10/models/epochs_8.pth', map_location='cpu')\n",
    "ck_backbone = torch.load('/nas/tsgil/relationformer/work_dirs/R2U_Net_pretrain/runs/baseline_R2U_Net_pretrain_epoch200_10/models/epochs_200.pth', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'schedulaer_state_dict'])\n",
      "odict_keys(['RRCNN1.RCNN.0.conv.0.weight', 'RRCNN1.RCNN.0.conv.0.bias', 'RRCNN1.RCNN.0.conv.1.weight', 'RRCNN1.RCNN.0.conv.1.bias', 'RRCNN1.RCNN.0.conv.1.running_mean', 'RRCNN1.RCNN.0.conv.1.running_var', 'RRCNN1.RCNN.0.conv.1.num_batches_tracked', 'RRCNN1.RCNN.1.conv.0.weight', 'RRCNN1.RCNN.1.conv.0.bias', 'RRCNN1.RCNN.1.conv.1.weight', 'RRCNN1.RCNN.1.conv.1.bias', 'RRCNN1.RCNN.1.conv.1.running_mean', 'RRCNN1.RCNN.1.conv.1.running_var', 'RRCNN1.RCNN.1.conv.1.num_batches_tracked', 'RRCNN1.Conv_1x1.weight', 'RRCNN1.Conv_1x1.bias', 'RRCNN2.RCNN.0.conv.0.weight', 'RRCNN2.RCNN.0.conv.0.bias', 'RRCNN2.RCNN.0.conv.1.weight', 'RRCNN2.RCNN.0.conv.1.bias', 'RRCNN2.RCNN.0.conv.1.running_mean', 'RRCNN2.RCNN.0.conv.1.running_var', 'RRCNN2.RCNN.0.conv.1.num_batches_tracked', 'RRCNN2.RCNN.1.conv.0.weight', 'RRCNN2.RCNN.1.conv.0.bias', 'RRCNN2.RCNN.1.conv.1.weight', 'RRCNN2.RCNN.1.conv.1.bias', 'RRCNN2.RCNN.1.conv.1.running_mean', 'RRCNN2.RCNN.1.conv.1.running_var', 'RRCNN2.RCNN.1.conv.1.num_batches_tracked', 'RRCNN2.Conv_1x1.weight', 'RRCNN2.Conv_1x1.bias', 'RRCNN3.RCNN.0.conv.0.weight', 'RRCNN3.RCNN.0.conv.0.bias', 'RRCNN3.RCNN.0.conv.1.weight', 'RRCNN3.RCNN.0.conv.1.bias', 'RRCNN3.RCNN.0.conv.1.running_mean', 'RRCNN3.RCNN.0.conv.1.running_var', 'RRCNN3.RCNN.0.conv.1.num_batches_tracked', 'RRCNN3.RCNN.1.conv.0.weight', 'RRCNN3.RCNN.1.conv.0.bias', 'RRCNN3.RCNN.1.conv.1.weight', 'RRCNN3.RCNN.1.conv.1.bias', 'RRCNN3.RCNN.1.conv.1.running_mean', 'RRCNN3.RCNN.1.conv.1.running_var', 'RRCNN3.RCNN.1.conv.1.num_batches_tracked', 'RRCNN3.Conv_1x1.weight', 'RRCNN3.Conv_1x1.bias', 'RRCNN4.RCNN.0.conv.0.weight', 'RRCNN4.RCNN.0.conv.0.bias', 'RRCNN4.RCNN.0.conv.1.weight', 'RRCNN4.RCNN.0.conv.1.bias', 'RRCNN4.RCNN.0.conv.1.running_mean', 'RRCNN4.RCNN.0.conv.1.running_var', 'RRCNN4.RCNN.0.conv.1.num_batches_tracked', 'RRCNN4.RCNN.1.conv.0.weight', 'RRCNN4.RCNN.1.conv.0.bias', 'RRCNN4.RCNN.1.conv.1.weight', 'RRCNN4.RCNN.1.conv.1.bias', 'RRCNN4.RCNN.1.conv.1.running_mean', 'RRCNN4.RCNN.1.conv.1.running_var', 'RRCNN4.RCNN.1.conv.1.num_batches_tracked', 'RRCNN4.Conv_1x1.weight', 'RRCNN4.Conv_1x1.bias', 'RRCNN5.RCNN.0.conv.0.weight', 'RRCNN5.RCNN.0.conv.0.bias', 'RRCNN5.RCNN.0.conv.1.weight', 'RRCNN5.RCNN.0.conv.1.bias', 'RRCNN5.RCNN.0.conv.1.running_mean', 'RRCNN5.RCNN.0.conv.1.running_var', 'RRCNN5.RCNN.0.conv.1.num_batches_tracked', 'RRCNN5.RCNN.1.conv.0.weight', 'RRCNN5.RCNN.1.conv.0.bias', 'RRCNN5.RCNN.1.conv.1.weight', 'RRCNN5.RCNN.1.conv.1.bias', 'RRCNN5.RCNN.1.conv.1.running_mean', 'RRCNN5.RCNN.1.conv.1.running_var', 'RRCNN5.RCNN.1.conv.1.num_batches_tracked', 'RRCNN5.Conv_1x1.weight', 'RRCNN5.Conv_1x1.bias', 'Up5.up.1.weight', 'Up5.up.1.bias', 'Up5.up.2.weight', 'Up5.up.2.bias', 'Up5.up.2.running_mean', 'Up5.up.2.running_var', 'Up5.up.2.num_batches_tracked', 'Up_RRCNN5.RCNN.0.conv.0.weight', 'Up_RRCNN5.RCNN.0.conv.0.bias', 'Up_RRCNN5.RCNN.0.conv.1.weight', 'Up_RRCNN5.RCNN.0.conv.1.bias', 'Up_RRCNN5.RCNN.0.conv.1.running_mean', 'Up_RRCNN5.RCNN.0.conv.1.running_var', 'Up_RRCNN5.RCNN.0.conv.1.num_batches_tracked', 'Up_RRCNN5.RCNN.1.conv.0.weight', 'Up_RRCNN5.RCNN.1.conv.0.bias', 'Up_RRCNN5.RCNN.1.conv.1.weight', 'Up_RRCNN5.RCNN.1.conv.1.bias', 'Up_RRCNN5.RCNN.1.conv.1.running_mean', 'Up_RRCNN5.RCNN.1.conv.1.running_var', 'Up_RRCNN5.RCNN.1.conv.1.num_batches_tracked', 'Up_RRCNN5.Conv_1x1.weight', 'Up_RRCNN5.Conv_1x1.bias', 'Up4.up.1.weight', 'Up4.up.1.bias', 'Up4.up.2.weight', 'Up4.up.2.bias', 'Up4.up.2.running_mean', 'Up4.up.2.running_var', 'Up4.up.2.num_batches_tracked', 'Up_RRCNN4.RCNN.0.conv.0.weight', 'Up_RRCNN4.RCNN.0.conv.0.bias', 'Up_RRCNN4.RCNN.0.conv.1.weight', 'Up_RRCNN4.RCNN.0.conv.1.bias', 'Up_RRCNN4.RCNN.0.conv.1.running_mean', 'Up_RRCNN4.RCNN.0.conv.1.running_var', 'Up_RRCNN4.RCNN.0.conv.1.num_batches_tracked', 'Up_RRCNN4.RCNN.1.conv.0.weight', 'Up_RRCNN4.RCNN.1.conv.0.bias', 'Up_RRCNN4.RCNN.1.conv.1.weight', 'Up_RRCNN4.RCNN.1.conv.1.bias', 'Up_RRCNN4.RCNN.1.conv.1.running_mean', 'Up_RRCNN4.RCNN.1.conv.1.running_var', 'Up_RRCNN4.RCNN.1.conv.1.num_batches_tracked', 'Up_RRCNN4.Conv_1x1.weight', 'Up_RRCNN4.Conv_1x1.bias', 'Up3.up.1.weight', 'Up3.up.1.bias', 'Up3.up.2.weight', 'Up3.up.2.bias', 'Up3.up.2.running_mean', 'Up3.up.2.running_var', 'Up3.up.2.num_batches_tracked', 'Up_RRCNN3.RCNN.0.conv.0.weight', 'Up_RRCNN3.RCNN.0.conv.0.bias', 'Up_RRCNN3.RCNN.0.conv.1.weight', 'Up_RRCNN3.RCNN.0.conv.1.bias', 'Up_RRCNN3.RCNN.0.conv.1.running_mean', 'Up_RRCNN3.RCNN.0.conv.1.running_var', 'Up_RRCNN3.RCNN.0.conv.1.num_batches_tracked', 'Up_RRCNN3.RCNN.1.conv.0.weight', 'Up_RRCNN3.RCNN.1.conv.0.bias', 'Up_RRCNN3.RCNN.1.conv.1.weight', 'Up_RRCNN3.RCNN.1.conv.1.bias', 'Up_RRCNN3.RCNN.1.conv.1.running_mean', 'Up_RRCNN3.RCNN.1.conv.1.running_var', 'Up_RRCNN3.RCNN.1.conv.1.num_batches_tracked', 'Up_RRCNN3.Conv_1x1.weight', 'Up_RRCNN3.Conv_1x1.bias', 'Up2.up.1.weight', 'Up2.up.1.bias', 'Up2.up.2.weight', 'Up2.up.2.bias', 'Up2.up.2.running_mean', 'Up2.up.2.running_var', 'Up2.up.2.num_batches_tracked', 'Up_RRCNN2.RCNN.0.conv.0.weight', 'Up_RRCNN2.RCNN.0.conv.0.bias', 'Up_RRCNN2.RCNN.0.conv.1.weight', 'Up_RRCNN2.RCNN.0.conv.1.bias', 'Up_RRCNN2.RCNN.0.conv.1.running_mean', 'Up_RRCNN2.RCNN.0.conv.1.running_var', 'Up_RRCNN2.RCNN.0.conv.1.num_batches_tracked', 'Up_RRCNN2.RCNN.1.conv.0.weight', 'Up_RRCNN2.RCNN.1.conv.0.bias', 'Up_RRCNN2.RCNN.1.conv.1.weight', 'Up_RRCNN2.RCNN.1.conv.1.bias', 'Up_RRCNN2.RCNN.1.conv.1.running_mean', 'Up_RRCNN2.RCNN.1.conv.1.running_var', 'Up_RRCNN2.RCNN.1.conv.1.num_batches_tracked', 'Up_RRCNN2.Conv_1x1.weight', 'Up_RRCNN2.Conv_1x1.bias', 'Detection.conv.0.weight', 'Detection.conv.0.bias', 'Detection.conv.1.weight', 'Detection.conv.1.bias', 'Detection.conv.1.running_mean', 'Detection.conv.1.running_var', 'Detection.conv.1.num_batches_tracked', 'Detection.conv.3.weight', 'Detection.conv.3.bias'])\n",
      "tensor([[[[-0.0071, -0.0069, -0.0093],\n",
      "          [-0.0065,  0.0175,  0.0231],\n",
      "          [ 0.0139,  0.0404, -0.0162]],\n",
      "\n",
      "         [[ 0.0101, -0.0107,  0.0387],\n",
      "          [-0.0205, -0.0426,  0.0300],\n",
      "          [ 0.0368, -0.0272,  0.0384]],\n",
      "\n",
      "         [[-0.0316,  0.0150, -0.0093],\n",
      "          [-0.0364, -0.0343, -0.0262],\n",
      "          [-0.0385, -0.0329, -0.0173]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0286,  0.0007, -0.0186],\n",
      "          [-0.0094,  0.0327, -0.0039],\n",
      "          [-0.0156, -0.0315,  0.0084]],\n",
      "\n",
      "         [[-0.0218, -0.0423, -0.0077],\n",
      "          [-0.0337, -0.0186, -0.0309],\n",
      "          [-0.0402, -0.0094,  0.0111]],\n",
      "\n",
      "         [[ 0.0202, -0.0004, -0.0208],\n",
      "          [ 0.0137,  0.0174, -0.0231],\n",
      "          [-0.0029,  0.0297,  0.0265]]],\n",
      "\n",
      "\n",
      "        [[[-0.0284, -0.0308,  0.0187],\n",
      "          [ 0.0024,  0.0369,  0.0255],\n",
      "          [-0.0065,  0.0170, -0.0431]],\n",
      "\n",
      "         [[ 0.0278, -0.0437,  0.0326],\n",
      "          [ 0.0137,  0.0292, -0.0021],\n",
      "          [-0.0162, -0.0398, -0.0398]],\n",
      "\n",
      "         [[-0.0360, -0.0268, -0.0350],\n",
      "          [-0.0209, -0.0136, -0.0423],\n",
      "          [-0.0025, -0.0270,  0.0320]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0210,  0.0099,  0.0348],\n",
      "          [-0.0144, -0.0404, -0.0120],\n",
      "          [-0.0324,  0.0210,  0.0026]],\n",
      "\n",
      "         [[-0.0067, -0.0513,  0.0375],\n",
      "          [-0.0038,  0.0169, -0.0124],\n",
      "          [-0.0009, -0.0098,  0.0182]],\n",
      "\n",
      "         [[-0.0311,  0.0373, -0.0270],\n",
      "          [ 0.0201,  0.0021, -0.0305],\n",
      "          [ 0.0239, -0.0103,  0.0145]]],\n",
      "\n",
      "\n",
      "        [[[-0.0053,  0.0046,  0.0146],\n",
      "          [ 0.0411,  0.0099, -0.0042],\n",
      "          [ 0.0306,  0.0322,  0.0238]],\n",
      "\n",
      "         [[ 0.0303,  0.0064,  0.0163],\n",
      "          [-0.0009, -0.0230,  0.0160],\n",
      "          [-0.0312, -0.0140,  0.0198]],\n",
      "\n",
      "         [[ 0.0064, -0.0245,  0.0376],\n",
      "          [-0.0163,  0.0231, -0.0182],\n",
      "          [-0.0114, -0.0092,  0.0091]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0087,  0.0210, -0.0446],\n",
      "          [ 0.0313, -0.0123, -0.0074],\n",
      "          [-0.0314,  0.0012, -0.0279]],\n",
      "\n",
      "         [[-0.0035, -0.0138,  0.0143],\n",
      "          [ 0.0024,  0.0383,  0.0178],\n",
      "          [-0.0216,  0.0178, -0.0231]],\n",
      "\n",
      "         [[ 0.0217, -0.0080, -0.0215],\n",
      "          [-0.0174,  0.0106, -0.0106],\n",
      "          [-0.0465,  0.0097, -0.0430]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0480,  0.0298,  0.0104],\n",
      "          [ 0.0235,  0.0398,  0.0298],\n",
      "          [ 0.0258, -0.0192,  0.0128]],\n",
      "\n",
      "         [[ 0.0041,  0.0186, -0.0292],\n",
      "          [ 0.0214,  0.0093, -0.0035],\n",
      "          [-0.0147, -0.0364, -0.0289]],\n",
      "\n",
      "         [[ 0.0084, -0.0334,  0.0114],\n",
      "          [-0.0254, -0.0368, -0.0147],\n",
      "          [-0.0156, -0.0068, -0.0193]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0371,  0.0249,  0.0489],\n",
      "          [ 0.0334,  0.0351, -0.0111],\n",
      "          [ 0.0024,  0.0219, -0.0173]],\n",
      "\n",
      "         [[ 0.0055, -0.0025,  0.0218],\n",
      "          [ 0.0461,  0.0045,  0.0168],\n",
      "          [-0.0130, -0.0170, -0.0135]],\n",
      "\n",
      "         [[-0.0174,  0.0203,  0.0029],\n",
      "          [-0.0129, -0.0008, -0.0282],\n",
      "          [-0.0187,  0.0299,  0.0244]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0240, -0.0081, -0.0193],\n",
      "          [ 0.0061,  0.0363,  0.0312],\n",
      "          [ 0.0295,  0.0357, -0.0032]],\n",
      "\n",
      "         [[-0.0337, -0.0033,  0.0272],\n",
      "          [ 0.0096,  0.0367, -0.0236],\n",
      "          [-0.0029, -0.0064,  0.0047]],\n",
      "\n",
      "         [[-0.0045,  0.0286,  0.0250],\n",
      "          [-0.0212, -0.0159, -0.0347],\n",
      "          [ 0.0286, -0.0220,  0.0027]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0242, -0.0140,  0.0108],\n",
      "          [-0.0033, -0.0152,  0.0105],\n",
      "          [-0.0217,  0.0309, -0.0241]],\n",
      "\n",
      "         [[ 0.0245, -0.0161,  0.0373],\n",
      "          [-0.0099,  0.0207, -0.0449],\n",
      "          [-0.0471,  0.0279,  0.0191]],\n",
      "\n",
      "         [[ 0.0150,  0.0175, -0.0163],\n",
      "          [ 0.0315,  0.0417, -0.0065],\n",
      "          [-0.0130,  0.0300,  0.0101]]],\n",
      "\n",
      "\n",
      "        [[[-0.0339,  0.0120, -0.0164],\n",
      "          [-0.0278, -0.0400,  0.0235],\n",
      "          [-0.0345,  0.0097,  0.0029]],\n",
      "\n",
      "         [[-0.0298,  0.0380,  0.0432],\n",
      "          [-0.0191,  0.0268,  0.0058],\n",
      "          [-0.0225,  0.0345, -0.0169]],\n",
      "\n",
      "         [[ 0.0222, -0.0019, -0.0133],\n",
      "          [ 0.0362,  0.0290,  0.0079],\n",
      "          [ 0.0369,  0.0264,  0.0163]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0395,  0.0310, -0.0215],\n",
      "          [ 0.0063,  0.0146, -0.0206],\n",
      "          [-0.0322,  0.0301,  0.0239]],\n",
      "\n",
      "         [[ 0.0033,  0.0175, -0.0222],\n",
      "          [-0.0211, -0.0049, -0.0308],\n",
      "          [-0.0423, -0.0139,  0.0413]],\n",
      "\n",
      "         [[-0.0115, -0.0069, -0.0375],\n",
      "          [ 0.0434, -0.0022, -0.0284],\n",
      "          [-0.0304,  0.0160,  0.0232]]]])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'model_state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m model3 \u001b[38;5;241m=\u001b[39m R2U_Net_origin()\n\u001b[1;32m      8\u001b[0m model1_dict \u001b[38;5;241m=\u001b[39m model1\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel1_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder.RRCNN1.RCNN.0.conv.0.weight\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     10\u001b[0m model1\u001b[38;5;241m.\u001b[39mload_state_dict(ck_backbone[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m], strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m total_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model1\u001b[38;5;241m.\u001b[39mparameters())\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model_state_dict'"
     ]
    }
   ],
   "source": [
    "print(ck_backbone.keys())\n",
    "print(ck_backbone['model_state_dict'].keys())\n",
    "print(ck_backbone['model_state_dict']['RRCNN1.RCNN.0.conv.0.weight'])\n",
    "\n",
    "model1 = build_TopDiG('/nas/tsgil/relationformer/configs/TopDiG_train.yaml')\n",
    "model2 = R2U_Net()\n",
    "model3 = R2U_Net_origin()\n",
    "model1_dict = model1.state_dict()\n",
    "print(model1_dict['model_state_dict']['encoder.RRCNN1.RCNN.0.conv.0.weight'])\n",
    "model1.load_state_dict(ck_backbone['model_state_dict'], strict=False)\n",
    "total_params = sum(p.numel() for p in model1.parameters())\n",
    "model3_dict = model3.state_dict()\n",
    "model2_dict = model2.state_dict()\n",
    "model1_dict = model1.state_dict()\n",
    "print(model1_dict['model_state_dict']['encoder.RRCNN1.RCNN.0.conv.0.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172 odict_keys(['RRCNN1.RCNN.0.conv.0.weight', 'RRCNN1.RCNN.0.conv.0.bias', 'RRCNN1.RCNN.0.conv.1.weight', 'RRCNN1.RCNN.0.conv.1.bias', 'RRCNN1.RCNN.0.conv.1.running_mean', 'RRCNN1.RCNN.0.conv.1.running_var', 'RRCNN1.RCNN.0.conv.1.num_batches_tracked', 'RRCNN1.RCNN.1.conv.0.weight', 'RRCNN1.RCNN.1.conv.0.bias', 'RRCNN1.RCNN.1.conv.1.weight', 'RRCNN1.RCNN.1.conv.1.bias', 'RRCNN1.RCNN.1.conv.1.running_mean', 'RRCNN1.RCNN.1.conv.1.running_var', 'RRCNN1.RCNN.1.conv.1.num_batches_tracked', 'RRCNN1.Conv_1x1.weight', 'RRCNN1.Conv_1x1.bias', 'RRCNN2.RCNN.0.conv.0.weight', 'RRCNN2.RCNN.0.conv.0.bias', 'RRCNN2.RCNN.0.conv.1.weight', 'RRCNN2.RCNN.0.conv.1.bias', 'RRCNN2.RCNN.0.conv.1.running_mean', 'RRCNN2.RCNN.0.conv.1.running_var', 'RRCNN2.RCNN.0.conv.1.num_batches_tracked', 'RRCNN2.RCNN.1.conv.0.weight', 'RRCNN2.RCNN.1.conv.0.bias', 'RRCNN2.RCNN.1.conv.1.weight', 'RRCNN2.RCNN.1.conv.1.bias', 'RRCNN2.RCNN.1.conv.1.running_mean', 'RRCNN2.RCNN.1.conv.1.running_var', 'RRCNN2.RCNN.1.conv.1.num_batches_tracked', 'RRCNN2.Conv_1x1.weight', 'RRCNN2.Conv_1x1.bias', 'RRCNN3.RCNN.0.conv.0.weight', 'RRCNN3.RCNN.0.conv.0.bias', 'RRCNN3.RCNN.0.conv.1.weight', 'RRCNN3.RCNN.0.conv.1.bias', 'RRCNN3.RCNN.0.conv.1.running_mean', 'RRCNN3.RCNN.0.conv.1.running_var', 'RRCNN3.RCNN.0.conv.1.num_batches_tracked', 'RRCNN3.RCNN.1.conv.0.weight', 'RRCNN3.RCNN.1.conv.0.bias', 'RRCNN3.RCNN.1.conv.1.weight', 'RRCNN3.RCNN.1.conv.1.bias', 'RRCNN3.RCNN.1.conv.1.running_mean', 'RRCNN3.RCNN.1.conv.1.running_var', 'RRCNN3.RCNN.1.conv.1.num_batches_tracked', 'RRCNN3.Conv_1x1.weight', 'RRCNN3.Conv_1x1.bias', 'RRCNN4.RCNN.0.conv.0.weight', 'RRCNN4.RCNN.0.conv.0.bias', 'RRCNN4.RCNN.0.conv.1.weight', 'RRCNN4.RCNN.0.conv.1.bias', 'RRCNN4.RCNN.0.conv.1.running_mean', 'RRCNN4.RCNN.0.conv.1.running_var', 'RRCNN4.RCNN.0.conv.1.num_batches_tracked', 'RRCNN4.RCNN.1.conv.0.weight', 'RRCNN4.RCNN.1.conv.0.bias', 'RRCNN4.RCNN.1.conv.1.weight', 'RRCNN4.RCNN.1.conv.1.bias', 'RRCNN4.RCNN.1.conv.1.running_mean', 'RRCNN4.RCNN.1.conv.1.running_var', 'RRCNN4.RCNN.1.conv.1.num_batches_tracked', 'RRCNN4.Conv_1x1.weight', 'RRCNN4.Conv_1x1.bias', 'RRCNN5.RCNN.0.conv.0.weight', 'RRCNN5.RCNN.0.conv.0.bias', 'RRCNN5.RCNN.0.conv.1.weight', 'RRCNN5.RCNN.0.conv.1.bias', 'RRCNN5.RCNN.0.conv.1.running_mean', 'RRCNN5.RCNN.0.conv.1.running_var', 'RRCNN5.RCNN.0.conv.1.num_batches_tracked', 'RRCNN5.RCNN.1.conv.0.weight', 'RRCNN5.RCNN.1.conv.0.bias', 'RRCNN5.RCNN.1.conv.1.weight', 'RRCNN5.RCNN.1.conv.1.bias', 'RRCNN5.RCNN.1.conv.1.running_mean', 'RRCNN5.RCNN.1.conv.1.running_var', 'RRCNN5.RCNN.1.conv.1.num_batches_tracked', 'RRCNN5.Conv_1x1.weight', 'RRCNN5.Conv_1x1.bias', 'Up5.up.1.weight', 'Up5.up.1.bias', 'Up5.up.2.weight', 'Up5.up.2.bias', 'Up5.up.2.running_mean', 'Up5.up.2.running_var', 'Up5.up.2.num_batches_tracked', 'Up_RRCNN5.RCNN.0.conv.0.weight', 'Up_RRCNN5.RCNN.0.conv.0.bias', 'Up_RRCNN5.RCNN.0.conv.1.weight', 'Up_RRCNN5.RCNN.0.conv.1.bias', 'Up_RRCNN5.RCNN.0.conv.1.running_mean', 'Up_RRCNN5.RCNN.0.conv.1.running_var', 'Up_RRCNN5.RCNN.0.conv.1.num_batches_tracked', 'Up_RRCNN5.RCNN.1.conv.0.weight', 'Up_RRCNN5.RCNN.1.conv.0.bias', 'Up_RRCNN5.RCNN.1.conv.1.weight', 'Up_RRCNN5.RCNN.1.conv.1.bias', 'Up_RRCNN5.RCNN.1.conv.1.running_mean', 'Up_RRCNN5.RCNN.1.conv.1.running_var', 'Up_RRCNN5.RCNN.1.conv.1.num_batches_tracked', 'Up_RRCNN5.Conv_1x1.weight', 'Up_RRCNN5.Conv_1x1.bias', 'Up4.up.1.weight', 'Up4.up.1.bias', 'Up4.up.2.weight', 'Up4.up.2.bias', 'Up4.up.2.running_mean', 'Up4.up.2.running_var', 'Up4.up.2.num_batches_tracked', 'Up_RRCNN4.RCNN.0.conv.0.weight', 'Up_RRCNN4.RCNN.0.conv.0.bias', 'Up_RRCNN4.RCNN.0.conv.1.weight', 'Up_RRCNN4.RCNN.0.conv.1.bias', 'Up_RRCNN4.RCNN.0.conv.1.running_mean', 'Up_RRCNN4.RCNN.0.conv.1.running_var', 'Up_RRCNN4.RCNN.0.conv.1.num_batches_tracked', 'Up_RRCNN4.RCNN.1.conv.0.weight', 'Up_RRCNN4.RCNN.1.conv.0.bias', 'Up_RRCNN4.RCNN.1.conv.1.weight', 'Up_RRCNN4.RCNN.1.conv.1.bias', 'Up_RRCNN4.RCNN.1.conv.1.running_mean', 'Up_RRCNN4.RCNN.1.conv.1.running_var', 'Up_RRCNN4.RCNN.1.conv.1.num_batches_tracked', 'Up_RRCNN4.Conv_1x1.weight', 'Up_RRCNN4.Conv_1x1.bias', 'Up3.up.1.weight', 'Up3.up.1.bias', 'Up3.up.2.weight', 'Up3.up.2.bias', 'Up3.up.2.running_mean', 'Up3.up.2.running_var', 'Up3.up.2.num_batches_tracked', 'Up_RRCNN3.RCNN.0.conv.0.weight', 'Up_RRCNN3.RCNN.0.conv.0.bias', 'Up_RRCNN3.RCNN.0.conv.1.weight', 'Up_RRCNN3.RCNN.0.conv.1.bias', 'Up_RRCNN3.RCNN.0.conv.1.running_mean', 'Up_RRCNN3.RCNN.0.conv.1.running_var', 'Up_RRCNN3.RCNN.0.conv.1.num_batches_tracked', 'Up_RRCNN3.RCNN.1.conv.0.weight', 'Up_RRCNN3.RCNN.1.conv.0.bias', 'Up_RRCNN3.RCNN.1.conv.1.weight', 'Up_RRCNN3.RCNN.1.conv.1.bias', 'Up_RRCNN3.RCNN.1.conv.1.running_mean', 'Up_RRCNN3.RCNN.1.conv.1.running_var', 'Up_RRCNN3.RCNN.1.conv.1.num_batches_tracked', 'Up_RRCNN3.Conv_1x1.weight', 'Up_RRCNN3.Conv_1x1.bias', 'Up2.up.1.weight', 'Up2.up.1.bias', 'Up2.up.2.weight', 'Up2.up.2.bias', 'Up2.up.2.running_mean', 'Up2.up.2.running_var', 'Up2.up.2.num_batches_tracked', 'Up_RRCNN2.RCNN.0.conv.0.weight', 'Up_RRCNN2.RCNN.0.conv.0.bias', 'Up_RRCNN2.RCNN.0.conv.1.weight', 'Up_RRCNN2.RCNN.0.conv.1.bias', 'Up_RRCNN2.RCNN.0.conv.1.running_mean', 'Up_RRCNN2.RCNN.0.conv.1.running_var', 'Up_RRCNN2.RCNN.0.conv.1.num_batches_tracked', 'Up_RRCNN2.RCNN.1.conv.0.weight', 'Up_RRCNN2.RCNN.1.conv.0.bias', 'Up_RRCNN2.RCNN.1.conv.1.weight', 'Up_RRCNN2.RCNN.1.conv.1.bias', 'Up_RRCNN2.RCNN.1.conv.1.running_mean', 'Up_RRCNN2.RCNN.1.conv.1.running_var', 'Up_RRCNN2.RCNN.1.conv.1.num_batches_tracked', 'Up_RRCNN2.Conv_1x1.weight', 'Up_RRCNN2.Conv_1x1.bias'])\n",
      "181 odict_keys(['RRCNN1.RCNN.0.conv.0.weight', 'RRCNN1.RCNN.0.conv.0.bias', 'RRCNN1.RCNN.0.conv.1.weight', 'RRCNN1.RCNN.0.conv.1.bias', 'RRCNN1.RCNN.0.conv.1.running_mean', 'RRCNN1.RCNN.0.conv.1.running_var', 'RRCNN1.RCNN.0.conv.1.num_batches_tracked', 'RRCNN1.RCNN.1.conv.0.weight', 'RRCNN1.RCNN.1.conv.0.bias', 'RRCNN1.RCNN.1.conv.1.weight', 'RRCNN1.RCNN.1.conv.1.bias', 'RRCNN1.RCNN.1.conv.1.running_mean', 'RRCNN1.RCNN.1.conv.1.running_var', 'RRCNN1.RCNN.1.conv.1.num_batches_tracked', 'RRCNN1.Conv_1x1.weight', 'RRCNN1.Conv_1x1.bias', 'RRCNN2.RCNN.0.conv.0.weight', 'RRCNN2.RCNN.0.conv.0.bias', 'RRCNN2.RCNN.0.conv.1.weight', 'RRCNN2.RCNN.0.conv.1.bias', 'RRCNN2.RCNN.0.conv.1.running_mean', 'RRCNN2.RCNN.0.conv.1.running_var', 'RRCNN2.RCNN.0.conv.1.num_batches_tracked', 'RRCNN2.RCNN.1.conv.0.weight', 'RRCNN2.RCNN.1.conv.0.bias', 'RRCNN2.RCNN.1.conv.1.weight', 'RRCNN2.RCNN.1.conv.1.bias', 'RRCNN2.RCNN.1.conv.1.running_mean', 'RRCNN2.RCNN.1.conv.1.running_var', 'RRCNN2.RCNN.1.conv.1.num_batches_tracked', 'RRCNN2.Conv_1x1.weight', 'RRCNN2.Conv_1x1.bias', 'RRCNN3.RCNN.0.conv.0.weight', 'RRCNN3.RCNN.0.conv.0.bias', 'RRCNN3.RCNN.0.conv.1.weight', 'RRCNN3.RCNN.0.conv.1.bias', 'RRCNN3.RCNN.0.conv.1.running_mean', 'RRCNN3.RCNN.0.conv.1.running_var', 'RRCNN3.RCNN.0.conv.1.num_batches_tracked', 'RRCNN3.RCNN.1.conv.0.weight', 'RRCNN3.RCNN.1.conv.0.bias', 'RRCNN3.RCNN.1.conv.1.weight', 'RRCNN3.RCNN.1.conv.1.bias', 'RRCNN3.RCNN.1.conv.1.running_mean', 'RRCNN3.RCNN.1.conv.1.running_var', 'RRCNN3.RCNN.1.conv.1.num_batches_tracked', 'RRCNN3.Conv_1x1.weight', 'RRCNN3.Conv_1x1.bias', 'RRCNN4.RCNN.0.conv.0.weight', 'RRCNN4.RCNN.0.conv.0.bias', 'RRCNN4.RCNN.0.conv.1.weight', 'RRCNN4.RCNN.0.conv.1.bias', 'RRCNN4.RCNN.0.conv.1.running_mean', 'RRCNN4.RCNN.0.conv.1.running_var', 'RRCNN4.RCNN.0.conv.1.num_batches_tracked', 'RRCNN4.RCNN.1.conv.0.weight', 'RRCNN4.RCNN.1.conv.0.bias', 'RRCNN4.RCNN.1.conv.1.weight', 'RRCNN4.RCNN.1.conv.1.bias', 'RRCNN4.RCNN.1.conv.1.running_mean', 'RRCNN4.RCNN.1.conv.1.running_var', 'RRCNN4.RCNN.1.conv.1.num_batches_tracked', 'RRCNN4.Conv_1x1.weight', 'RRCNN4.Conv_1x1.bias', 'RRCNN5.RCNN.0.conv.0.weight', 'RRCNN5.RCNN.0.conv.0.bias', 'RRCNN5.RCNN.0.conv.1.weight', 'RRCNN5.RCNN.0.conv.1.bias', 'RRCNN5.RCNN.0.conv.1.running_mean', 'RRCNN5.RCNN.0.conv.1.running_var', 'RRCNN5.RCNN.0.conv.1.num_batches_tracked', 'RRCNN5.RCNN.1.conv.0.weight', 'RRCNN5.RCNN.1.conv.0.bias', 'RRCNN5.RCNN.1.conv.1.weight', 'RRCNN5.RCNN.1.conv.1.bias', 'RRCNN5.RCNN.1.conv.1.running_mean', 'RRCNN5.RCNN.1.conv.1.running_var', 'RRCNN5.RCNN.1.conv.1.num_batches_tracked', 'RRCNN5.Conv_1x1.weight', 'RRCNN5.Conv_1x1.bias', 'Up5.up.1.weight', 'Up5.up.1.bias', 'Up5.up.2.weight', 'Up5.up.2.bias', 'Up5.up.2.running_mean', 'Up5.up.2.running_var', 'Up5.up.2.num_batches_tracked', 'Up_RRCNN5.RCNN.0.conv.0.weight', 'Up_RRCNN5.RCNN.0.conv.0.bias', 'Up_RRCNN5.RCNN.0.conv.1.weight', 'Up_RRCNN5.RCNN.0.conv.1.bias', 'Up_RRCNN5.RCNN.0.conv.1.running_mean', 'Up_RRCNN5.RCNN.0.conv.1.running_var', 'Up_RRCNN5.RCNN.0.conv.1.num_batches_tracked', 'Up_RRCNN5.RCNN.1.conv.0.weight', 'Up_RRCNN5.RCNN.1.conv.0.bias', 'Up_RRCNN5.RCNN.1.conv.1.weight', 'Up_RRCNN5.RCNN.1.conv.1.bias', 'Up_RRCNN5.RCNN.1.conv.1.running_mean', 'Up_RRCNN5.RCNN.1.conv.1.running_var', 'Up_RRCNN5.RCNN.1.conv.1.num_batches_tracked', 'Up_RRCNN5.Conv_1x1.weight', 'Up_RRCNN5.Conv_1x1.bias', 'Up4.up.1.weight', 'Up4.up.1.bias', 'Up4.up.2.weight', 'Up4.up.2.bias', 'Up4.up.2.running_mean', 'Up4.up.2.running_var', 'Up4.up.2.num_batches_tracked', 'Up_RRCNN4.RCNN.0.conv.0.weight', 'Up_RRCNN4.RCNN.0.conv.0.bias', 'Up_RRCNN4.RCNN.0.conv.1.weight', 'Up_RRCNN4.RCNN.0.conv.1.bias', 'Up_RRCNN4.RCNN.0.conv.1.running_mean', 'Up_RRCNN4.RCNN.0.conv.1.running_var', 'Up_RRCNN4.RCNN.0.conv.1.num_batches_tracked', 'Up_RRCNN4.RCNN.1.conv.0.weight', 'Up_RRCNN4.RCNN.1.conv.0.bias', 'Up_RRCNN4.RCNN.1.conv.1.weight', 'Up_RRCNN4.RCNN.1.conv.1.bias', 'Up_RRCNN4.RCNN.1.conv.1.running_mean', 'Up_RRCNN4.RCNN.1.conv.1.running_var', 'Up_RRCNN4.RCNN.1.conv.1.num_batches_tracked', 'Up_RRCNN4.Conv_1x1.weight', 'Up_RRCNN4.Conv_1x1.bias', 'Up3.up.1.weight', 'Up3.up.1.bias', 'Up3.up.2.weight', 'Up3.up.2.bias', 'Up3.up.2.running_mean', 'Up3.up.2.running_var', 'Up3.up.2.num_batches_tracked', 'Up_RRCNN3.RCNN.0.conv.0.weight', 'Up_RRCNN3.RCNN.0.conv.0.bias', 'Up_RRCNN3.RCNN.0.conv.1.weight', 'Up_RRCNN3.RCNN.0.conv.1.bias', 'Up_RRCNN3.RCNN.0.conv.1.running_mean', 'Up_RRCNN3.RCNN.0.conv.1.running_var', 'Up_RRCNN3.RCNN.0.conv.1.num_batches_tracked', 'Up_RRCNN3.RCNN.1.conv.0.weight', 'Up_RRCNN3.RCNN.1.conv.0.bias', 'Up_RRCNN3.RCNN.1.conv.1.weight', 'Up_RRCNN3.RCNN.1.conv.1.bias', 'Up_RRCNN3.RCNN.1.conv.1.running_mean', 'Up_RRCNN3.RCNN.1.conv.1.running_var', 'Up_RRCNN3.RCNN.1.conv.1.num_batches_tracked', 'Up_RRCNN3.Conv_1x1.weight', 'Up_RRCNN3.Conv_1x1.bias', 'Up2.up.1.weight', 'Up2.up.1.bias', 'Up2.up.2.weight', 'Up2.up.2.bias', 'Up2.up.2.running_mean', 'Up2.up.2.running_var', 'Up2.up.2.num_batches_tracked', 'Up_RRCNN2.RCNN.0.conv.0.weight', 'Up_RRCNN2.RCNN.0.conv.0.bias', 'Up_RRCNN2.RCNN.0.conv.1.weight', 'Up_RRCNN2.RCNN.0.conv.1.bias', 'Up_RRCNN2.RCNN.0.conv.1.running_mean', 'Up_RRCNN2.RCNN.0.conv.1.running_var', 'Up_RRCNN2.RCNN.0.conv.1.num_batches_tracked', 'Up_RRCNN2.RCNN.1.conv.0.weight', 'Up_RRCNN2.RCNN.1.conv.0.bias', 'Up_RRCNN2.RCNN.1.conv.1.weight', 'Up_RRCNN2.RCNN.1.conv.1.bias', 'Up_RRCNN2.RCNN.1.conv.1.running_mean', 'Up_RRCNN2.RCNN.1.conv.1.running_var', 'Up_RRCNN2.RCNN.1.conv.1.num_batches_tracked', 'Up_RRCNN2.Conv_1x1.weight', 'Up_RRCNN2.Conv_1x1.bias', 'Detection.conv.0.weight', 'Detection.conv.0.bias', 'Detection.conv.1.weight', 'Detection.conv.1.bias', 'Detection.conv.1.running_mean', 'Detection.conv.1.running_var', 'Detection.conv.1.num_batches_tracked', 'Detection.conv.3.weight', 'Detection.conv.3.bias'])\n",
      "260 odict_keys(['encoder.RRCNN1.RCNN.0.conv.0.weight', 'encoder.RRCNN1.RCNN.0.conv.0.bias', 'encoder.RRCNN1.RCNN.0.conv.1.weight', 'encoder.RRCNN1.RCNN.0.conv.1.bias', 'encoder.RRCNN1.RCNN.0.conv.1.running_mean', 'encoder.RRCNN1.RCNN.0.conv.1.running_var', 'encoder.RRCNN1.RCNN.0.conv.1.num_batches_tracked', 'encoder.RRCNN1.RCNN.1.conv.0.weight', 'encoder.RRCNN1.RCNN.1.conv.0.bias', 'encoder.RRCNN1.RCNN.1.conv.1.weight', 'encoder.RRCNN1.RCNN.1.conv.1.bias', 'encoder.RRCNN1.RCNN.1.conv.1.running_mean', 'encoder.RRCNN1.RCNN.1.conv.1.running_var', 'encoder.RRCNN1.RCNN.1.conv.1.num_batches_tracked', 'encoder.RRCNN1.Conv_1x1.weight', 'encoder.RRCNN1.Conv_1x1.bias', 'encoder.RRCNN2.RCNN.0.conv.0.weight', 'encoder.RRCNN2.RCNN.0.conv.0.bias', 'encoder.RRCNN2.RCNN.0.conv.1.weight', 'encoder.RRCNN2.RCNN.0.conv.1.bias', 'encoder.RRCNN2.RCNN.0.conv.1.running_mean', 'encoder.RRCNN2.RCNN.0.conv.1.running_var', 'encoder.RRCNN2.RCNN.0.conv.1.num_batches_tracked', 'encoder.RRCNN2.RCNN.1.conv.0.weight', 'encoder.RRCNN2.RCNN.1.conv.0.bias', 'encoder.RRCNN2.RCNN.1.conv.1.weight', 'encoder.RRCNN2.RCNN.1.conv.1.bias', 'encoder.RRCNN2.RCNN.1.conv.1.running_mean', 'encoder.RRCNN2.RCNN.1.conv.1.running_var', 'encoder.RRCNN2.RCNN.1.conv.1.num_batches_tracked', 'encoder.RRCNN2.Conv_1x1.weight', 'encoder.RRCNN2.Conv_1x1.bias', 'encoder.RRCNN3.RCNN.0.conv.0.weight', 'encoder.RRCNN3.RCNN.0.conv.0.bias', 'encoder.RRCNN3.RCNN.0.conv.1.weight', 'encoder.RRCNN3.RCNN.0.conv.1.bias', 'encoder.RRCNN3.RCNN.0.conv.1.running_mean', 'encoder.RRCNN3.RCNN.0.conv.1.running_var', 'encoder.RRCNN3.RCNN.0.conv.1.num_batches_tracked', 'encoder.RRCNN3.RCNN.1.conv.0.weight', 'encoder.RRCNN3.RCNN.1.conv.0.bias', 'encoder.RRCNN3.RCNN.1.conv.1.weight', 'encoder.RRCNN3.RCNN.1.conv.1.bias', 'encoder.RRCNN3.RCNN.1.conv.1.running_mean', 'encoder.RRCNN3.RCNN.1.conv.1.running_var', 'encoder.RRCNN3.RCNN.1.conv.1.num_batches_tracked', 'encoder.RRCNN3.Conv_1x1.weight', 'encoder.RRCNN3.Conv_1x1.bias', 'encoder.RRCNN4.RCNN.0.conv.0.weight', 'encoder.RRCNN4.RCNN.0.conv.0.bias', 'encoder.RRCNN4.RCNN.0.conv.1.weight', 'encoder.RRCNN4.RCNN.0.conv.1.bias', 'encoder.RRCNN4.RCNN.0.conv.1.running_mean', 'encoder.RRCNN4.RCNN.0.conv.1.running_var', 'encoder.RRCNN4.RCNN.0.conv.1.num_batches_tracked', 'encoder.RRCNN4.RCNN.1.conv.0.weight', 'encoder.RRCNN4.RCNN.1.conv.0.bias', 'encoder.RRCNN4.RCNN.1.conv.1.weight', 'encoder.RRCNN4.RCNN.1.conv.1.bias', 'encoder.RRCNN4.RCNN.1.conv.1.running_mean', 'encoder.RRCNN4.RCNN.1.conv.1.running_var', 'encoder.RRCNN4.RCNN.1.conv.1.num_batches_tracked', 'encoder.RRCNN4.Conv_1x1.weight', 'encoder.RRCNN4.Conv_1x1.bias', 'encoder.RRCNN5.RCNN.0.conv.0.weight', 'encoder.RRCNN5.RCNN.0.conv.0.bias', 'encoder.RRCNN5.RCNN.0.conv.1.weight', 'encoder.RRCNN5.RCNN.0.conv.1.bias', 'encoder.RRCNN5.RCNN.0.conv.1.running_mean', 'encoder.RRCNN5.RCNN.0.conv.1.running_var', 'encoder.RRCNN5.RCNN.0.conv.1.num_batches_tracked', 'encoder.RRCNN5.RCNN.1.conv.0.weight', 'encoder.RRCNN5.RCNN.1.conv.0.bias', 'encoder.RRCNN5.RCNN.1.conv.1.weight', 'encoder.RRCNN5.RCNN.1.conv.1.bias', 'encoder.RRCNN5.RCNN.1.conv.1.running_mean', 'encoder.RRCNN5.RCNN.1.conv.1.running_var', 'encoder.RRCNN5.RCNN.1.conv.1.num_batches_tracked', 'encoder.RRCNN5.Conv_1x1.weight', 'encoder.RRCNN5.Conv_1x1.bias', 'encoder.Up5.up.1.weight', 'encoder.Up5.up.1.bias', 'encoder.Up5.up.2.weight', 'encoder.Up5.up.2.bias', 'encoder.Up5.up.2.running_mean', 'encoder.Up5.up.2.running_var', 'encoder.Up5.up.2.num_batches_tracked', 'encoder.Up_RRCNN5.RCNN.0.conv.0.weight', 'encoder.Up_RRCNN5.RCNN.0.conv.0.bias', 'encoder.Up_RRCNN5.RCNN.0.conv.1.weight', 'encoder.Up_RRCNN5.RCNN.0.conv.1.bias', 'encoder.Up_RRCNN5.RCNN.0.conv.1.running_mean', 'encoder.Up_RRCNN5.RCNN.0.conv.1.running_var', 'encoder.Up_RRCNN5.RCNN.0.conv.1.num_batches_tracked', 'encoder.Up_RRCNN5.RCNN.1.conv.0.weight', 'encoder.Up_RRCNN5.RCNN.1.conv.0.bias', 'encoder.Up_RRCNN5.RCNN.1.conv.1.weight', 'encoder.Up_RRCNN5.RCNN.1.conv.1.bias', 'encoder.Up_RRCNN5.RCNN.1.conv.1.running_mean', 'encoder.Up_RRCNN5.RCNN.1.conv.1.running_var', 'encoder.Up_RRCNN5.RCNN.1.conv.1.num_batches_tracked', 'encoder.Up_RRCNN5.Conv_1x1.weight', 'encoder.Up_RRCNN5.Conv_1x1.bias', 'encoder.Up4.up.1.weight', 'encoder.Up4.up.1.bias', 'encoder.Up4.up.2.weight', 'encoder.Up4.up.2.bias', 'encoder.Up4.up.2.running_mean', 'encoder.Up4.up.2.running_var', 'encoder.Up4.up.2.num_batches_tracked', 'encoder.Up_RRCNN4.RCNN.0.conv.0.weight', 'encoder.Up_RRCNN4.RCNN.0.conv.0.bias', 'encoder.Up_RRCNN4.RCNN.0.conv.1.weight', 'encoder.Up_RRCNN4.RCNN.0.conv.1.bias', 'encoder.Up_RRCNN4.RCNN.0.conv.1.running_mean', 'encoder.Up_RRCNN4.RCNN.0.conv.1.running_var', 'encoder.Up_RRCNN4.RCNN.0.conv.1.num_batches_tracked', 'encoder.Up_RRCNN4.RCNN.1.conv.0.weight', 'encoder.Up_RRCNN4.RCNN.1.conv.0.bias', 'encoder.Up_RRCNN4.RCNN.1.conv.1.weight', 'encoder.Up_RRCNN4.RCNN.1.conv.1.bias', 'encoder.Up_RRCNN4.RCNN.1.conv.1.running_mean', 'encoder.Up_RRCNN4.RCNN.1.conv.1.running_var', 'encoder.Up_RRCNN4.RCNN.1.conv.1.num_batches_tracked', 'encoder.Up_RRCNN4.Conv_1x1.weight', 'encoder.Up_RRCNN4.Conv_1x1.bias', 'encoder.Up3.up.1.weight', 'encoder.Up3.up.1.bias', 'encoder.Up3.up.2.weight', 'encoder.Up3.up.2.bias', 'encoder.Up3.up.2.running_mean', 'encoder.Up3.up.2.running_var', 'encoder.Up3.up.2.num_batches_tracked', 'encoder.Up_RRCNN3.RCNN.0.conv.0.weight', 'encoder.Up_RRCNN3.RCNN.0.conv.0.bias', 'encoder.Up_RRCNN3.RCNN.0.conv.1.weight', 'encoder.Up_RRCNN3.RCNN.0.conv.1.bias', 'encoder.Up_RRCNN3.RCNN.0.conv.1.running_mean', 'encoder.Up_RRCNN3.RCNN.0.conv.1.running_var', 'encoder.Up_RRCNN3.RCNN.0.conv.1.num_batches_tracked', 'encoder.Up_RRCNN3.RCNN.1.conv.0.weight', 'encoder.Up_RRCNN3.RCNN.1.conv.0.bias', 'encoder.Up_RRCNN3.RCNN.1.conv.1.weight', 'encoder.Up_RRCNN3.RCNN.1.conv.1.bias', 'encoder.Up_RRCNN3.RCNN.1.conv.1.running_mean', 'encoder.Up_RRCNN3.RCNN.1.conv.1.running_var', 'encoder.Up_RRCNN3.RCNN.1.conv.1.num_batches_tracked', 'encoder.Up_RRCNN3.Conv_1x1.weight', 'encoder.Up_RRCNN3.Conv_1x1.bias', 'encoder.Up2.up.1.weight', 'encoder.Up2.up.1.bias', 'encoder.Up2.up.2.weight', 'encoder.Up2.up.2.bias', 'encoder.Up2.up.2.running_mean', 'encoder.Up2.up.2.running_var', 'encoder.Up2.up.2.num_batches_tracked', 'encoder.Up_RRCNN2.RCNN.0.conv.0.weight', 'encoder.Up_RRCNN2.RCNN.0.conv.0.bias', 'encoder.Up_RRCNN2.RCNN.0.conv.1.weight', 'encoder.Up_RRCNN2.RCNN.0.conv.1.bias', 'encoder.Up_RRCNN2.RCNN.0.conv.1.running_mean', 'encoder.Up_RRCNN2.RCNN.0.conv.1.running_var', 'encoder.Up_RRCNN2.RCNN.0.conv.1.num_batches_tracked', 'encoder.Up_RRCNN2.RCNN.1.conv.0.weight', 'encoder.Up_RRCNN2.RCNN.1.conv.0.bias', 'encoder.Up_RRCNN2.RCNN.1.conv.1.weight', 'encoder.Up_RRCNN2.RCNN.1.conv.1.bias', 'encoder.Up_RRCNN2.RCNN.1.conv.1.running_mean', 'encoder.Up_RRCNN2.RCNN.1.conv.1.running_var', 'encoder.Up_RRCNN2.RCNN.1.conv.1.num_batches_tracked', 'encoder.Up_RRCNN2.Conv_1x1.weight', 'encoder.Up_RRCNN2.Conv_1x1.bias', 'decoder.mlp.fc1.weight', 'decoder.mlp.fc1.bias', 'decoder.mlp.bn1.weight', 'decoder.mlp.bn1.bias', 'decoder.mlp.bn1.running_mean', 'decoder.mlp.bn1.running_var', 'decoder.mlp.bn1.num_batches_tracked', 'decoder.mlp.fc2.weight', 'decoder.mlp.fc2.bias', 'decoder.connectionNet.transformer_encoder.layers.0.self_attn.in_proj_weight', 'decoder.connectionNet.transformer_encoder.layers.0.self_attn.in_proj_bias', 'decoder.connectionNet.transformer_encoder.layers.0.self_attn.out_proj.weight', 'decoder.connectionNet.transformer_encoder.layers.0.self_attn.out_proj.bias', 'decoder.connectionNet.transformer_encoder.layers.0.linear1.weight', 'decoder.connectionNet.transformer_encoder.layers.0.linear1.bias', 'decoder.connectionNet.transformer_encoder.layers.0.linear2.weight', 'decoder.connectionNet.transformer_encoder.layers.0.linear2.bias', 'decoder.connectionNet.transformer_encoder.layers.0.norm1.weight', 'decoder.connectionNet.transformer_encoder.layers.0.norm1.bias', 'decoder.connectionNet.transformer_encoder.layers.0.norm2.weight', 'decoder.connectionNet.transformer_encoder.layers.0.norm2.bias', 'decoder.connectionNet.transformer_encoder.layers.1.self_attn.in_proj_weight', 'decoder.connectionNet.transformer_encoder.layers.1.self_attn.in_proj_bias', 'decoder.connectionNet.transformer_encoder.layers.1.self_attn.out_proj.weight', 'decoder.connectionNet.transformer_encoder.layers.1.self_attn.out_proj.bias', 'decoder.connectionNet.transformer_encoder.layers.1.linear1.weight', 'decoder.connectionNet.transformer_encoder.layers.1.linear1.bias', 'decoder.connectionNet.transformer_encoder.layers.1.linear2.weight', 'decoder.connectionNet.transformer_encoder.layers.1.linear2.bias', 'decoder.connectionNet.transformer_encoder.layers.1.norm1.weight', 'decoder.connectionNet.transformer_encoder.layers.1.norm1.bias', 'decoder.connectionNet.transformer_encoder.layers.1.norm2.weight', 'decoder.connectionNet.transformer_encoder.layers.1.norm2.bias', 'decoder.graphHead1.conv1.weight', 'decoder.graphHead1.conv1.bias', 'decoder.graphHead1.bn1.weight', 'decoder.graphHead1.bn1.bias', 'decoder.graphHead1.bn1.running_mean', 'decoder.graphHead1.bn1.running_var', 'decoder.graphHead1.bn1.num_batches_tracked', 'decoder.graphHead1.conv2.weight', 'decoder.graphHead1.conv2.bias', 'decoder.graphHead1.bn2.weight', 'decoder.graphHead1.bn2.bias', 'decoder.graphHead1.bn2.running_mean', 'decoder.graphHead1.bn2.running_var', 'decoder.graphHead1.bn2.num_batches_tracked', 'decoder.graphHead1.conv3.weight', 'decoder.graphHead1.conv3.bias', 'decoder.graphHead1.bn3.weight', 'decoder.graphHead1.bn3.bias', 'decoder.graphHead1.bn3.running_mean', 'decoder.graphHead1.bn3.running_var', 'decoder.graphHead1.bn3.num_batches_tracked', 'decoder.graphHead1.conv4.weight', 'decoder.graphHead1.conv4.bias', 'decoder.graphHead2.conv1.weight', 'decoder.graphHead2.conv1.bias', 'decoder.graphHead2.bn1.weight', 'decoder.graphHead2.bn1.bias', 'decoder.graphHead2.bn1.running_mean', 'decoder.graphHead2.bn1.running_var', 'decoder.graphHead2.bn1.num_batches_tracked', 'decoder.graphHead2.conv2.weight', 'decoder.graphHead2.conv2.bias', 'decoder.graphHead2.bn2.weight', 'decoder.graphHead2.bn2.bias', 'decoder.graphHead2.bn2.running_mean', 'decoder.graphHead2.bn2.running_var', 'decoder.graphHead2.bn2.num_batches_tracked', 'decoder.graphHead2.conv3.weight', 'decoder.graphHead2.conv3.bias', 'decoder.graphHead2.bn3.weight', 'decoder.graphHead2.bn3.bias', 'decoder.graphHead2.bn3.running_mean', 'decoder.graphHead2.bn3.running_var', 'decoder.graphHead2.bn3.num_batches_tracked', 'decoder.graphHead2.conv4.weight', 'decoder.graphHead2.conv4.bias', 'detectionBranch.conv.0.weight', 'detectionBranch.conv.0.bias', 'detectionBranch.conv.1.weight', 'detectionBranch.conv.1.bias', 'detectionBranch.conv.1.running_mean', 'detectionBranch.conv.1.running_var', 'detectionBranch.conv.1.num_batches_tracked', 'detectionBranch.conv.3.weight', 'detectionBranch.conv.3.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(len(model3_dict.keys()), model3_dict.keys())\n",
    "print(len(model2_dict.keys()), model2_dict.keys())\n",
    "print(len(model1_dict.keys()), model1_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
